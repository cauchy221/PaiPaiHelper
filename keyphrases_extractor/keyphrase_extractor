{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"mount_file_id":"1Tqh9Ury-xuHLh9qwR07QmuDNSjMRgtP4","authorship_tag":"ABX9TyNkafrj5NjMddcEWDgHBsRT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!pip install torch==1.8.1\n","!pip install ltp==4.1.4\n","!pip install thulac==0.2.1\n","!pip install nltk==3.5\n","!pip install transformers==4.9.2\n","!pip install sentence_transformers==2.0.0\n"],"metadata":{"id":"BT4j3ooFaqL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 扫描到相应的路径\n","import sys\n","sys.path.append('/content/drive/MyDrive/paipai-NLP/keyphrases_extractor')"],"metadata":{"id":"MTVrl09Kd1Ur","executionInfo":{"status":"ok","timestamp":1662526064031,"user_tz":-480,"elapsed":4,"user":{"displayName":"刘欣悦","userId":"07269853958112221817"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"IiJHf0WtWQtp","executionInfo":{"status":"ok","timestamp":1662526211506,"user_tz":-480,"elapsed":1255,"user":{"displayName":"刘欣悦","userId":"07269853958112221817"}}},"outputs":[],"source":["import time\n","import numpy as np\n","import thulac\n","import nltk\n","from nltk.corpus import stopwords\n","from ltp import LTP\n","\n","import torch\n","import torch.nn.functional as F\n","from transformers import ElectraModel, ElectraTokenizerFast\n","from sentence_transformers.util import pytorch_cos_sim\n","\n","from utils import get_word_weight, process_long_input, rematch\n","\n","\n","# 报错替换\n","# import collections.abc as container_abcs\n","# int_classes = int\n","# string_classes = str"]},{"cell_type":"code","source":["english_punctuations = [',', '.', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%']\n","chinese_punctuations = '！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.'\n","punctuations = ''.join(i for i in english_punctuations) + chinese_punctuations\n","\n","\n","# 注意只对英文去停，中文停用词保留\n","nltk.download('stopwords')\n","stop_words = stopwords.words('english')"],"metadata":{"id":"e5L09lNXWSW7","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1662526215093,"user_tz":-480,"elapsed":2,"user":{"displayName":"刘欣悦","userId":"07269853958112221817"}},"outputId":"6835611b-09d4-49ff-b0aa-d0333f0eb12e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","source":["weightfile_pretrain = '/content/drive/MyDrive/paipai-NLP/keyphrases_extractor/resources/pretrained_weight_dict.txt'\n","weightpara_pretrain = 2.7e-4\n","word2weight_pretrain = get_word_weight(weightfile_pretrain, weightpara_pretrain)"],"metadata":{"id":"8V5NGLDiWVIp","executionInfo":{"status":"ok","timestamp":1662526222729,"user_tz":-480,"elapsed":4437,"user":{"displayName":"刘欣悦","userId":"07269853958112221817"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["lac_model = thulac.thulac()\n","\n","ltp_model_path = 'base'\n","ltp_ner_usr_dict_path = '/content/drive/MyDrive/paipai-NLP/keyphrases_extractor/resources/ner_usr_dict.txt'\n","\n","usr_dict = []\n","with open(ltp_ner_usr_dict_path) as f:\n","    for line in f.readlines():\n","        usr_dict.append(line.split('\\n')[0])"],"metadata":{"id":"K3_QZhydWlzd","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1662526230792,"user_tz":-480,"elapsed":3464,"user":{"displayName":"刘欣悦","userId":"07269853958112221817"}},"outputId":"5f322c1b-4667-42b5-d5e3-3aa229d57576"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Model loaded succeed\n"]}]},{"cell_type":"code","source":["class LTPForTokenizeAndPostag:\n","\t\"\"\"\n","\t用于分词和词性分析\n","\t---------------\n","\tver: 2021-11-01\n","\tby: changhongyu\n","\t\"\"\"\n","\tdef __init__(self, ltp_model_path, ners=None, device='cpu'):\n","\t\t\"\"\"\n","\t\t:param ltp_model_path: str: ltp模型的路径\n","\t\t:param ners: list: 用户输入的实体列表\n","\t\t:param device: str: cpu还是cuda\n","\t\t\"\"\"\n","\t\tprint('Initializing LTP model from {}.'.format(ltp_model_path))\n","\t\tself.ltp_model = LTP(ltp_model_path, device=device)\n","\t\tprint('LTP model created.')\n","\t\tif ners:\n","\t\t\tself.ltp_model.add_words(words=ners)\n","\t\t# 为了保持与thu-lac模型的词性标记形式一致，做了这个映射\n","\t\t# 当然，也可以不映射，然后对3.2.4的抽取器进行适当修改\n","\t\tself.ltp_to_lac_pos_map = {\n","\t\t\t\t\t\t\t\t   'b': 'a',\n","\t\t\t\t\t\t\t\t   'nd': 'f',\n","\t\t\t\t\t\t\t\t   'nh': 'np',\n","\t\t\t\t\t\t\t\t   'nl': 'ns',\n","\t\t\t\t\t\t\t\t   'nt': 't',\n","\t\t\t\t\t\t\t\t   'wp': 'w',\n","\t\t\t\t\t\t\t\t   'ws': 'x',\n","\t\t\t\t\t\t\t  \t  }\n","\n","\tdef _get_tokens(self, text):\n","\t\ttokens, hidden = self.ltp_model.seg(self.ltp_model.sent_split([text]))\n","\t\tself.hidden = hidden\n","\t\tpara_tokens = []\n","\t\tfor t in tokens:\n","\t\t\tpara_tokens += t\n","\t\t\n","\t\treturn para_tokens\n","\tdef _get_pos(self, text):\n","\t\ttags = self.ltp_model.pos(self.hidden)\n","\t\tpara_tags = []\n","\t\tfor t in tags:\n","\t\t\tpara_tags += t\n","\t\t\n","\t\treturn para_tags\n","\t\n","\t# 因为lac模型的调用方法是cut，所以保持一致用cut命名\n","\tdef cut(self, text):\n","\t\t\"\"\"\n","\t\t:param text: str: 输入文本\n","\t\t:return token_list: list: tokenized\n","\t\t:return token_tag_list: list: token对应的词性\n","\t\t\"\"\"\n","\t\ttoken_list = self._get_tokens(text)\n","\t\ttoken_tag_list = self._get_pos(text)\n","\t\tassert len(token_list) == len(token_tag_list)\n","\t\ttoken_tag_list_lac = []\n","\t\tfor tag in token_tag_list:\n","\t\t\tif tag in self.ltp_to_lac_pos_map:\n","\t\t\t\ttoken_tag_list_lac.append(self.ltp_to_lac_pos_map[tag])\n","\t\t\telse:\n","\t\t\t\ttoken_tag_list_lac.append(tag)\n","\t\t\t\n","\t\treturn [[token, tag] for token, tag in zip(token_list, token_tag_list_lac)]"],"metadata":{"id":"9_0TRjyGWq_x","executionInfo":{"status":"ok","timestamp":1662527082626,"user_tz":-480,"elapsed":619,"user":{"displayName":"刘欣悦","userId":"07269853958112221817"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["ltp_pos_model = LTPForTokenizeAndPostag(ltp_model_path, ners=usr_dict, device='cuda:0')"],"metadata":{"id":"mhj3CmDVWs-7","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1662527088473,"user_tz":-480,"elapsed":1949,"user":{"displayName":"刘欣悦","userId":"07269853958112221817"}},"outputId":"9f94fbb9-9d40-47fd-8186-0633e9e904a7"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing LTP model from base.\n"]},{"output_type":"stream","name":"stderr","text":["file /root/.cache/torch/ltp/8909177e47aa4daf900c569b86053ac68838d09da28c7bbeb42b8efcb08f56aa-edb9303f86310d4bcfd1ac0fa20a744c9a7e13ee515fe3cf88ad31921ed616b2-extracted/config.json not found\n","file /root/.cache/torch/ltp/8909177e47aa4daf900c569b86053ac68838d09da28c7bbeb42b8efcb08f56aa-edb9303f86310d4bcfd1ac0fa20a744c9a7e13ee515fe3cf88ad31921ed616b2-extracted/config.json not found\n"]},{"output_type":"stream","name":"stdout","text":["LTP model created.\n"]}]},{"cell_type":"code","source":["class CandidateExtractor:\n","    \"\"\"\n","    参考SIFRank项目的词性正则抽取候选短语\n","    \"\"\"\n","    def __init__(self):\n","        grammar = \"\"\"  NP:\n","                    {<n.*|a|uw|i|j|x>*<n.*|uw|x>|<x|j><-><m|q>|<\"><n.*><\"><n.*>*<n.*>}\"\"\"\n","        self.parser = nltk.RegexpParser(grammar)\n","    \n","    def extract_candidates(self, tokens_tagged):\n","        keyphrase_candidate = []\n","        np_pos_tag_tokens = self.parser.parse(tokens_tagged)\n","        count = 0\n","        for token in np_pos_tag_tokens:\n","            if (isinstance(token, nltk.tree.Tree) and token._label == \"NP\"):\n","                np = ''.join(word for word, tag in token.leaves())\n","                length = len(token.leaves())\n","                start_end = (count, count + length)\n","                count += length\n","                keyphrase_candidate.append((np, start_end))\n","            else:\n","                count += 1\n","        \n","        return keyphrase_candidate\n","        \n","candidate_extractor = CandidateExtractor()"],"metadata":{"id":"Hq361GFyWv1n","executionInfo":{"status":"ok","timestamp":1662527090382,"user_tz":-480,"elapsed":575,"user":{"displayName":"刘欣悦","userId":"07269853958112221817"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["lemma_model = nltk.WordNetLemmatizer()"],"metadata":{"id":"bM-qLEjaWwnn","executionInfo":{"status":"ok","timestamp":1662527102075,"user_tz":-480,"elapsed":602,"user":{"displayName":"刘欣悦","userId":"07269853958112221817"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizerFast, BertModel\n","\n","bert_path = '/content/drive/MyDrive/paipai-NLP/keyphrases_extractor/resources/bert-base-chinese/'\n","bert_model = BertModel.from_pretrained(bert_path)\n","bert_tokenizer = BertTokenizerFast.from_pretrained(bert_path)"],"metadata":{"id":"CpG-yCyJWyDM","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1662527104134,"user_tz":-480,"elapsed":1202,"user":{"displayName":"刘欣悦","userId":"07269853958112221817"}},"outputId":"103e7e4b-0573-49f2-b68f-1ed4e573466f"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/paipai-NLP/keyphrases_extractor/resources/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","source":["class SIFRank:\n","    \"\"\"\n","    用于抽取关键短语的SIFRank模型\n","    [步骤]\n","    1. 对原句进行tokenize和词性标注\n","    2. 对原句进行编码，并根据1中tokenize的结果获取embedding_list\n","    3. 根据1中tokenize的结果获取weight_list\n","    4. 抽取原句中的候选关键短语\n","    5. 对候选关键短语进行评分，得到关键短语\n","    ---------------\n","    ver: 2021-11-02\n","    by: changhongyu\n","    \"\"\"\n","    def __init__(self, tokenize_and_postag_model, candidate_extractor, lemma_model,\n","                 encoding_model, encoding_tokenizer, encoding_pooling, encoding_device, \n","                 word2weight_pretrain, stop_words, punctuations):\n","        \"\"\"\n","        :param tokenize_and_postag_model: 分词和词性标注模型\n","        :param candidate_extractor: 用于抽取候选短语的模型\n","        :param lemma_model: 用于词根还原的模型, 如果None，则忽略\n","        :param encoding_model: PretrainedModel: 编码预训练模型\n","        :param encoding_tokenizer: PretrainedTokenizer: 编码时的tokenizer\n","        :param encoding_pooling: str: 编码时的池化策略, 'mean'或'max'\n","        :param encoding_device: str: 编码时的设备, 'cpu'或'cuda'\n","        :param word2weight_pretrain: dict: 词汇对应权重的大list\n","        :param stop_words: list: 停用词表\n","        :param punctuations: list: 标点符号表\n","        \"\"\"\n","        assert encoding_pooling in ['mean', 'max'], Exception(\"Pooling must be either mean or max.\")\n","        assert encoding_device.startswith('cuda') or encoding_device == 'cpu'\n","        self.tokenize_and_postag_model = tokenize_and_postag_model\n","        self.extractor = candidate_extractor\n","        self.lemma_model = lemma_model\n","        self.encoding_model = encoding_model\n","        self.encoding_tokenizer = encoding_tokenizer\n","        self.encoding_pooling = encoding_pooling\n","        self.encoding_device = torch.device(encoding_device)\n","        self.word2weight_pretrain = word2weight_pretrain\n","        self.stop_words = stop_words\n","        self.punctuations = punctuations\n","        print(self)\n","    \n","    def __repr__(self):\n","        infos = ['------SIFRank for key-phrase extract------\\n',\n","                 'SETTINGS: \\n'\n","                 'tokenize_and_postag_model:  {}\\n'.format(str(type(self.tokenize_and_postag_model)).replace(\"'>\", \"\").split('.')[-1]),\n","                 'lemma_model:  {}\\n'.format(str(type(self.lemma_model)).replace(\"'>\", \"\").split('.')[-1]),\n","                 'encoding_model:  {}\\n'.format(str(type(self.encoding_model)).replace(\"'>\", \"\").split('.')[-1]),\n","                 'encoding_device:  {}\\n'.format(self.encoding_device),\n","                 'encoding_pooling:  {}\\n'.format(self.encoding_pooling),\n","                ]\n","        \n","        return ''.join(info for info in infos)\n","    \n","    def add_stopword(self, stop_word):\n","        \"\"\"\n","        添加停用词，注意停用词是指英文停用词\n","        \"\"\"\n","        self.stop_words.append(stop_word)\n","        \n","    def add_punctuation(self, punctuation):\n","        \"\"\"\n","        添加标点符\n","        \"\"\"\n","        self.punctuations.append(punctuation)\n","    \n","    def _get_embedding_list(self, text, target_tokens):\n","        \"\"\"\n","        获取以token为划分的embedding的list\n","        TODO: 对原句进行清洗，过滤掉对encoding_tokenizer而言OOV的词(耗时太长)\n","        :param text: str: 原文\n","        :param target_tokens: list: tokenize_and_postag_model对当前输入的分词结果\n","        \"\"\"\n","        embedding_list = []\n","        self.encoding_model.to(self.encoding_device)\n","\n","        ## <1. 获取编码\n","        features = self.encoding_tokenizer(text.lower().replace(' ', '-'),\n","                                           max_length=1024,\n","                                           truncation=True,\n","                                           padding='longest',\n","                                           return_tensors='pt')\n","        input_ids = features['input_ids'].to(self.encoding_device)\n","        # token_type_ids = features['token_type_ids'].to(self.encoding_device)\n","        attention_mask = features['attention_mask'].to(self.encoding_device)\n","\n","        with torch.no_grad():\n","            # enconding_out = self.encoding_model(input_ids, token_type_ids, attention_mask)\n","            # last_hidden_state = enconding_out['last_hidden_state'].squeeze(0).detach().cpu().numpy()\n","            enconding_out, _ = process_long_input(self.encoding_model, \n","                                                  input_ids, \n","                                                  attention_mask, \n","                                                  [self.encoding_tokenizer.cls_token_id], \n","                                                  [self.encoding_tokenizer.sep_token_id])\n","            # last_hidden_state: (len, hidden)\n","            last_hidden_state = enconding_out.squeeze(0).detach().cpu().numpy()\n","\n","        ## 1>\n","\n","        ## <2. token对齐\n","        t_mapping = rematch(text, target_tokens, do_lower_case=True)\n","        s_mapping = rematch(text, self.encoding_tokenizer.tokenize(text), do_lower_case=True)\n","        \n","        token_lens = []\n","        t_pointer = 0\n","        t = t_mapping[t_pointer]\n","        cur_len = 0\n","        cur_in_t = 0\n","        for s in s_mapping:\n","            # print(s, t[cur_in_t: cur_in_t + len(s)])\n","            if s == t[cur_in_t: cur_in_t + len(s)]:\n","                cur_len += 1\n","                cur_in_t += len(s)\n","                if cur_in_t == len(t):\n","                    # 判断当前target结束\n","                    token_lens.append(cur_len)\n","                    cur_len = 0\n","                    cur_in_t = 0\n","                    t_pointer += 1\n","                    if t_pointer >= len(t_mapping):\n","                        break\n","                    t = t_mapping[t_pointer]\n","        ## 2>\n","        assert len(token_lens) == len(target_tokens), \\\n","                Exception(\"Token_lens and target_tokens shape unmatch: {} vs {}.\".format(len(token_lens), len(target_tokens)))\n","\n","        ## <3 根据token_len获取对应的embedding池化\n","        cur_pos = 0\n","        for token_len in token_lens:\n","            if token_len == 0:\n","                # 如果是空字符，则置为全零\n","                cur_emb = np.zeros(last_hidden_state.shape[1])\n","                embedding_list.append(cur_emb)\n","                continue\n","            if self.encoding_pooling == 'mean':\n","                cur_emb = np.mean(last_hidden_state[cur_pos: cur_pos + token_len][:], axis=0)\n","            elif self.encoding_pooling == 'max':\n","                cur_emb = np.max(last_hidden_state[cur_pos: cur_pos + token_len][:], axis=0)\n","            else:\n","                raise ValueError(\"Pooling Strategy must be either mean or max.\")\n","            cur_pos += token_len\n","            embedding_list.append(cur_emb)\n","        ## 3>\n","\n","        assert len(embedding_list) == len(target_tokens), \\\n","                Exception(\"Result embedding list must have same length as target.\")\n","\n","        return embedding_list\n","    \n","    def _get_weight_list(self, target_tokens):\n","        \"\"\"\n","        获取weight列表\n","        :param target_tokens: list: tokenize_and_postag_model对当前输入的分词结果\n","        :return weight_list: list of float: 每个token对应的预训练权重列表\n","        \"\"\"\n","        weight_list = []\n","        _max = 0.\n","        for token in target_tokens:\n","            token = token.lower()\n","            if token in self.stop_words or token in self.punctuations:\n","                weight = 0.\n","            elif token in self.word2weight_pretrain:\n","                weight = word2weight_pretrain[token]\n","            else:\n","                # 如果OOV，返回截至当前句中最大的token\n","                weight = _max\n","            _max = max(weight, _max)\n","            weight_list.append(weight)\n","        \n","        return weight_list\n","    \n","    def _get_candidate_list(self, target_tokens, target_poses):\n","        \"\"\"\n","        用词性正则抽取候选关键短语列表\n","        :param target_tokens: list: tokenize_and_postag_model对当前输入的分词结果\n","        :param target_poses: list: tokenize_and_postag_model对当前输入词性标注结果\n","        :return candidates: list of tuples like: ('自然语言', (5, 7))\n","            NOTE: tuple[1]是在target_tokens中的span，对target_tokens索引，得到tuple[0]\n","        \"\"\"\n","        assert len(target_tokens) == len(target_poses)\n","        tokens_tagged = [(tok, pos) for tok, pos in zip(target_tokens, target_poses)]\n","        candidates = self.extractor.extract_candidates(tokens_tagged)\n","        \n","        return candidates\n","    \n","    def _extract_keyphrase(self, candidates, weight_list, embedding_list, max_keyphrase_num):\n","        \"\"\"\n","        对候选的关键短语计算与原文编码的相似度，获取关键短语\n","        :param candidates: list of tuples: 候选关键短语list\n","        :param weight_list: list of float: 每个token的预训练权重列表\n","        :param embedding_list: list of array: 每个token的编码结果\n","        :param max_keyphrase_num: int: 最多保留的关键词个数\n","        :return key_phrases: list of tuple: [(k1, 0.9), ...]\n","        \"\"\"\n","        assert len(weight_list) == len(embedding_list)\n","        # 获取每个候选短语的编码\n","        candidate_embeddings_list = []\n","        for cand in candidates:\n","            cand_emb = self.get_candidate_weight_avg(weight_list, embedding_list, cand[1])\n","            candidate_embeddings_list.append(cand_emb)\n","            \n","        # 计算候选短语与原文的相似度\n","        sent_embeddings = self.get_candidate_weight_avg(weight_list, embedding_list, (0, len(embedding_list)))\n","        sim_list = []\n","        for i, emb in enumerate(candidate_embeddings_list):\n","            sim = float(pytorch_cos_sim(sent_embeddings, candidate_embeddings_list[i]).squeeze().numpy())\n","            sim_list.append(sim)\n","            \n","        # 对候选短语归并，词根相同的短语放在一起\n","        dict_all = {}\n","        for i, cand in enumerate(candidates):\n","            if self.lemma_model:\n","                cand_lemma = self.lemma_model.lemmatize(cand[0].lower()).replace('▲', ' ')\n","            else:\n","                cand_lemma = cand[0].lower().replace('▲', ' ')\n","            if cand_lemma in dict_all:\n","                dict_all[cand_lemma].append(sim_list[i])\n","            else:\n","                dict_all[cand_lemma] = [sim_list[i]]\n","        \n","        # 对归并结果求平均\n","        final_dict = {}\n","        for cand, sim_list in dict_all.items():\n","            sum_sim = sum(sim_list)\n","            final_dict[cand] = sum_sim / len(sim_list)\n","            \n","        return sorted(final_dict.items(), key=lambda x: x[1], reverse=True)[: max_keyphrase_num]\n","    \n","    def __call__(self, text, max_keyphrase_num):\n","        \"\"\"\n","        抽取关键词\n","        :param text: str: 待抽取原文\n","        :param max_keyphrase_num: int: 最多保留的关键词个数\n","        :return key_phrases: list of tuple: [(k1, 0.9), ...]\n","        \"\"\"\n","        text = self.preprocess_input_text(text)\n","        t0 = time.time()\n","        \n","        ## <1. 对原句进行tokenize和词性标注\n","        token_and_pos = self.tokenize_and_postag_model.cut(text)\n","        target_tokens = [t_p[0] for t_p in token_and_pos]\n","        target_poses = [t_p[1] for t_p in token_and_pos]\n","        \n","        for i, token in enumerate(target_tokens):\n","            if token in self.stop_words:\n","                target_poses[i] = \"u\"\n","            if token == '-':\n","                target_poses[i] = \"-\"\n","            if token in ['\"', \"'\"]:\n","                target_poses[i] = '\"'\n","                \n","        t1 = time.time()\n","        print(\"耗时统计\")\n","        print(\"<1. 对原句进行tokenize和词性标注: \", round(t1 - t0, 2), 's')\n","        ## 1>\n","        \n","        ## <2. 对原句进行编码，并根据1中tokenize的结果获取embedding_list\n","        embedding_list = self._get_embedding_list(text, target_tokens)\n","        t2 = time.time()\n","        print(\"<2. 对原句进行编码: \", round(t2 - t1, 2), 's')\n","        ## 2>\n","        \n","        ## <3. 根据1中tokenize的结果获取weight_list\n","        weight_list = self._get_weight_list(target_tokens)\n","        t3 = time.time()\n","        print(\"<3. 结果获取weight_list: \", round(t3 - t2, 2), 's')\n","        ## 3>\n","        \n","        ## <4. 抽取原句中的候选关键短语\n","        candidate_list = self._get_candidate_list(target_tokens, target_poses)\n","        t4 = time.time()\n","        print(\"<4. 抽取原句中的候选关键短语: \", round(t4 - t3, 2), 's')\n","        ## 4>\n","        \n","        ## <5. 对候选关键短语进行评分，得到关键短语\n","        key_phrases = self._extract_keyphrase(candidate_list, weight_list, \n","                                              embedding_list, max_keyphrase_num)\n","        t5 = time.time()\n","        print(\"<5. 对候选关键短语进行评分: \", round(t5 - t4, 2), 's')\n","        ## 5>\n","        \n","        return key_phrases\n","        \n","    @staticmethod\n","    def get_candidate_weight_avg(weight_list, embedding_list, candidate_span):\n","        \"\"\"\n","        获取一个候选词的加权表征\n","        :param weight_list: list of float: 每个token的预训练权重列表\n","        :param embedding_list: list of array: 每个token的编码结果\n","        :param candidate_span: tuple: 候选短语的start和end\n","        \"\"\"\n","        assert len(weight_list) == len(embedding_list)\n","        start, end = candidate_span\n","        num_words = end - start\n","        embedding_size = embedding_list[0].shape[0]\n","\n","        sum_ = np.zeros(embedding_size)\n","        for i in range(start, end):\n","            tmp = embedding_list[i] * weight_list[i]\n","            sum_ += tmp\n","        \n","        return sum_\n","    \n","    @staticmethod\n","    def preprocess_input_text(text):\n","        \"\"\"\n","        对输入原文进行预处理，主要防止两个tokenizer对齐时出现问题\n","        \"\"\"\n","        text = text.lower()\n","        # 全部判断过于耗时\n","        # text = ''.join(char for char in text if char in self.encoding_tokenizer.vocab)\n","        text = text.replace('“', '\"').replace('”', '\"')\n","        text = text.replace('‘', \"'\").replace('’', \"'\")\n","        text = text.replace('⁃', '-')\n","        text = text.replace('\\u3000', ' ').replace('\\n', ' ')\n","        text = text.replace(' ', '▲')\n","        # text = text.replace(' ', '¤')\n","        \n","        return text[: 1024]"],"metadata":{"id":"2L969aA4W24c","executionInfo":{"status":"ok","timestamp":1662527104134,"user_tz":-480,"elapsed":1,"user":{"displayName":"刘欣悦","userId":"07269853958112221817"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["keyphrase_extractor = SIFRank(tokenize_and_postag_model=ltp_pos_model,\n","                              candidate_extractor=candidate_extractor,\n","                              lemma_model=lemma_model,\n","                              encoding_model=bert_model,\n","                              encoding_tokenizer=bert_tokenizer,\n","                              encoding_pooling='mean',\n","                              encoding_device='cuda:0',\n","                              word2weight_pretrain=word2weight_pretrain,\n","                              stop_words=stop_words,\n","                              punctuations=punctuations)"],"metadata":{"id":"Iy3CPvpBXA4n","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1662527109223,"user_tz":-480,"elapsed":1,"user":{"displayName":"刘欣悦","userId":"07269853958112221817"}},"outputId":"3f8dc8ae-e08d-4c6e-b40a-d03a19f2cb2a"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["------SIFRank for key-phrase extract------\n","SETTINGS: \n","tokenize_and_postag_model:  LTPForTokenizeAndPostag\n","lemma_model:  WordNetLemmatizer\n","encoding_model:  BertModel\n","encoding_device:  cuda:0\n","encoding_pooling:  mean\n","\n"]}]},{"cell_type":"code","source":["text = \"根据团委要求，我们将在今天下午开展以冬奥会为主题的团日活动，看到的同学请回复。收到。收到。收到。收到。收到。收到。收到。收到。\\\n","此外，我们将在下周进行考试，请各位同学做好准备。好的。收到，会复习的！看这个视频，贼搞笑。哈哈哈哈哈哈哈哈哈哈。确实哈哈哈哈哈哈哈。明天大家一起去吃海底捞吧，新学期聚餐。好呀好呀。好耶！好好好\""],"metadata":{"id":"s0asPOuQXZYu","executionInfo":{"status":"ok","timestamp":1662527111181,"user_tz":-480,"elapsed":1,"user":{"displayName":"刘欣悦","userId":"07269853958112221817"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('wordnet')\n","keyphrase_extractor(text, max_keyphrase_num=10)"],"metadata":{"id":"lZGPVJoGXIiV","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1662527165510,"user_tz":-480,"elapsed":6295,"user":{"displayName":"刘欣悦","userId":"07269853958112221817"}},"outputId":"931489b6-ac5a-4f08-e8dd-b333c7d99b7c"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["耗时统计\n","<1. 对原句进行tokenize和词性标注:  0.1 s\n","<2. 对原句进行编码:  0.05 s\n","<3. 结果获取weight_list:  0.0 s\n","<4. 抽取原句中的候选关键短语:  0.0 s\n","<5. 对候选关键短语进行评分:  5.78 s\n"]},{"output_type":"execute_result","data":{"text/plain":["[('团委', 0.8050410466989801),\n"," ('团日活动', 0.7966873973671578),\n"," ('海底捞', 0.7905426801810123),\n"," ('新学期', 0.7822226500176613),\n"," ('同学', 0.7725323117496359),\n"," ('视频', 0.7683449751303018),\n"," ('主题', 0.7594762807214842)]"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["text2 = \"明天下午大家有时间吗，我们开个大班会。有时间。有。应该有。这个是传达学院工作精神的会议。请所有人确认活动告知了所有班级同学，确认完成后在本群回复。收到。收到。收到。收到。\""],"metadata":{"id":"hbMZuInGWAuL","executionInfo":{"status":"ok","timestamp":1662527424804,"user_tz":-480,"elapsed":1477,"user":{"displayName":"刘欣悦","userId":"07269853958112221817"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["keyphrase_extractor(text2, max_keyphrase_num=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"1I4flBItWxTq","executionInfo":{"status":"ok","timestamp":1662527425406,"user_tz":-480,"elapsed":2,"user":{"displayName":"刘欣悦","userId":"07269853958112221817"}},"outputId":"b89f1ede-ed56-49a4-d9cb-a675b274981b"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["耗时统计\n","<1. 对原句进行tokenize和词性标注:  0.07 s\n","<2. 对原句进行编码:  0.07 s\n","<3. 结果获取weight_list:  0.01 s\n","<4. 抽取原句中的候选关键短语:  0.0 s\n","<5. 对候选关键短语进行评分:  0.0 s\n"]},{"output_type":"execute_result","data":{"text/plain":["[('所有班级同学', 0.8746033033000243),\n"," ('大班会', 0.8427173391162208),\n"," ('会议', 0.8194923943920582),\n"," ('学院', 0.7954272957910615),\n"," ('所有人', 0.7843820857297764)]"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["text3 = '近日，俄罗斯海军在美国阿拉斯加州附近外海进行了涉及数十艘舰艇和飞机的大型演习，这是自冷战结束后在该地区举行的最大规模演习。\\n演习期间，一艘俄罗斯核潜艇在阿拉斯加外海突然上浮，这一不同寻常的举动引起了美军的高度关注。\\n据美联社8月29日报道，俄罗斯海军司令尼古拉·叶夫梅诺夫上将说，有50多艘军舰和约40架飞机参加了正在白令海举行的演习，演习中涉及多次导弹发射练习。\\n“瓦良格”号导弹巡洋舰发射“玄武岩”反舰导弹。\\n俄海军在白令海举行演习\\n叶夫梅诺夫在俄罗斯国防部发表的一份声明中说：“这是我们有史以来第一次在那里举行如此大规模的演习。”叶夫梅诺夫强调，这些演习是为了加强俄罗斯在北极地区的存在和保护俄罗斯的资源。他说：“我们正在建立我们的力量，以确保该地区的经济发展”，“ 我们正在适应北极。”\\n目前还不清楚演习何时开始，也不清楚演习是否已经结束\\n俄罗斯太平洋舰队参加了此次演习，该舰队表示，作为演习的一部分，“鄂木斯克”号核潜艇和“瓦良格”号导弹巡洋舰向白令海的一个练习目标发射了巡航导弹。演习还从楚科奇半岛海岸向阿纳德尔湾的一个练习目标发射了“玛瑙石”岸舰导弹。\\n“这两种舰艇都是冷战时期苏联针对美国航母研制的武器，因此此次演习的科目包括演练水面舰艇和潜艇联合打击航母等大型水面战舰。”军事专家韩东分析认为。\\n“瓦良格”号导弹巡洋舰是俄太平洋舰队的旗舰，配备了16枚“玄武岩”超声速反舰导弹，最大射程约500千米，而“鄂木斯克”号巡航导弹核潜艇则配备24枚“花岗岩”超声速反舰导弹，射程超过500千米。\\n在演习进行期间，美军27日发现一艘俄罗斯潜艇在阿拉斯加附近浮出水面。美国北方司令部发言人比尔·刘易斯指出，俄罗斯的军事演习是在美国境外的国际水域内进行的。刘易斯说，北美航空航天防御司令部和美国北方司令部正在密切监视这艘潜艇。他还说，他们还没有收到俄罗斯海军的任何援助请求，但随时准备为遇难者提供帮助。\\n俄罗斯国家通讯社援引俄罗斯太平洋舰队消息人士的话说，“鄂木斯克”号核潜艇浮出水面是例行公事。\\n除了海上的行动，同样是在27日晚些时候，北美防空司令部派出F-22战斗机拦截了靠近阿拉斯加的两架俄图-142海上巡逻机。俄军飞机在该地区停留了约5个小时。俄相关负责人说，俄军机仍在国际空域，任何时候都没有进入美国或加拿大主权空域。\\n对于俄军机的飞行行动。美国北美防空司令格伦·范赫克（Glen D。 VanHerck）将军在一份声明中说：“随着我们的竞争对手继续扩大军事存在并探测我们的防御，我们的北部方面力量增加了对外国军事活动的监视……今年，我们进行了十多次拦截，这是近年来最多的一次。我们继续努力在北部进行防空行动的重要性从未如此显著。”\\n俄军冷战后首次在位于俄罗斯远东领土与美国阿拉斯加州之间的白令海举行联合军事演习引发了外界的关注。俄新社援引俄罗斯海军前参谋长、退役上将维克多·克拉夫琴科评价说：“这是一个信号，表明我们没有沉睡，我们想去哪里就去哪里。”\\n美联社报道称，俄海军在白令海的演习引起了美国商业渔船的骚动。美国海岸警卫队发言人基普·瓦德洛27日表示：“我们接到多艘在白令海外作业的渔船的通知，他们遇到了俄军舰艇，并感到担忧。”海岸警卫队联系了位于埃尔曼多夫-理查森联合基地的阿拉斯加司令部，该司令部证实，出现在那里的船只（指俄军舰艇）是俄罗斯预先计划的军事演习的一部分，美国军方官员已经知道。\\n俄军苏-27拦截美国B-52H轰炸机。\\n美俄相互在对方“家门口”展示“肌肉”\\n俄罗斯海空军在阿拉斯加“家门口”的白令海举行演习，美国在俄罗斯“家门口”也有不少军事动作。\\n据美国“战区”网站8月25日报道，美国海军发布照片显示，美国“海狼”号核潜艇近日浮出水面，出现在挪威近海。据悉，“海狼”级核潜艇堪称美海军最强核潜艇，主要用于开展情报活动和执行特别任务，这次的公开露面非常罕见，其出没的海域是俄罗斯核潜艇从俄西北部基地前往大洋的必经之地。\\n“美海军很少公布执行任务的核潜艇行动信息，此次公布‘海狼’级核潜艇在挪威近海活动应该是对俄核潜艇在阿拉斯加外海活动的一个回应，相互亮‘肌肉’。”韩东认为。'"],"metadata":{"id":"g6xFB1dpW6ER","executionInfo":{"status":"ok","timestamp":1662527475875,"user_tz":-480,"elapsed":614,"user":{"displayName":"刘欣悦","userId":"07269853958112221817"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["keyphrase_extractor(text3, max_keyphrase_num=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"nA6oHiK3W95P","executionInfo":{"status":"ok","timestamp":1662527475875,"user_tz":-480,"elapsed":2,"user":{"displayName":"刘欣悦","userId":"07269853958112221817"}},"outputId":"ceb657b9-aa43-4dce-e200-3ef6d27e718e"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["耗时统计\n","<1. 对原句进行tokenize和词性标注:  0.24 s\n","<2. 对原句进行编码:  0.1 s\n","<3. 结果获取weight_list:  0.01 s\n","<4. 抽取原句中的候选关键短语:  0.0 s\n","<5. 对候选关键短语进行评分:  0.02 s\n"]},{"output_type":"execute_result","data":{"text/plain":["[('俄罗斯太平洋舰队消息人士', 0.9257313238431253),\n"," ('\"鄂木斯克\"号核潜艇', 0.9165496292692027),\n"," ('\"瓦良格\"号导弹巡洋舰', 0.9143536528266387),\n"," ('俄太平洋舰队', 0.9081027495948981),\n"," ('俄罗斯海军司令尼古拉·叶夫梅诺夫上将', 0.9076248207860832),\n"," ('俄罗斯核潜艇', 0.9073924189946807),\n"," ('司令部发言人比尔·刘易斯', 0.8999544048066915),\n"," ('\"鄂木斯克\"号', 0.8979974104490015),\n"," ('俄罗斯太平洋舰队', 0.8966218779002009),\n"," ('北美航空航天', 0.8939950394437334)]"]},"metadata":{},"execution_count":44}]}]}